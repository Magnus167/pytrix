{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AnztxsQmDV7J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from difflib import get_close_matches\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from joblib import Parallel, delayed\n",
        "import csv\n",
        "\n",
        "\n",
        "'''load train.csv'''\n",
        "train = pd.read_csv('data/train.csv', dtype=str)\n",
        "features = pd.read_csv('data/features.csv', dtype=str)\n",
        "'''load patient_notes.csv'''\n",
        "patient_notes = pd.read_csv('data/patient_notes.csv', dtype=str)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QMB7sby_DZLn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 1 ... 0 0 1]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[3285, 14547, 15929, 16824, 18895, 28154, 29012, 39042]\n"
          ]
        }
      ],
      "source": [
        "def strtypeArrToArr(strtypeArr):\n",
        "      return [x.strip() for x in eval(strtypeArr.upper())]\n",
        "\n",
        "def get_words_only(text):\n",
        "    return re.sub('[^a-zA-Z]+', ' ', ' '.join(text.split()).upper()).strip('\"').strip(\"'\").split()\n",
        "\n",
        "def get_words_only_str(text):\n",
        "    return ' '.join(get_words_only(text))\n",
        "\n",
        "def get_words_from_notes(patient_notes, train, save_file=True):\n",
        "    words_list = set()\n",
        "    for I in range(len(patient_notes)):\n",
        "        words_list.update(get_words_only(patient_notes['pn_history'][I].strip()))\n",
        "    for I in range(len(train)):\n",
        "        words_list.update(get_words_only(train['annotation'][I].strip()))\n",
        "\n",
        "    # words_list = sorted(list(set(re.sub('[^a-zA-Z]+', ' ', ' '.join(words_list)).split())))\n",
        "    words_list = sorted(list(words_list))\n",
        "    # words_list = sorted([word for word in tqdm(word_tokenize(' '.join(words_list))) if not word in stopwords.words()])\n",
        "\n",
        "    if save_file:\n",
        "        with open('data/words_list.txt', 'w') as f:\n",
        "            for item in words_list:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "\n",
        "    return words_list\n",
        "        \n",
        "def encode_to_arr(text, words_list, return_long=True):\n",
        "    r = [0] * len(words_list)\n",
        "    words = get_words_only(text)\n",
        "    for word in words:\n",
        "        try:\n",
        "            r[words_list.index(word)] = 1\n",
        "        except ValueError:\n",
        "            # possible_words = get_close_matches('HYROID', get_words_from_notes(patient_notes), cutoff=0.9)\n",
        "            pass\n",
        "    return np.array(r) if return_long else np.array(np.where(np.array(r) == 1)).tolist()[0]\n",
        "\n",
        "def encode_labels(label, labels_list, return_long=True, normalize=False):\n",
        "    r = [0] * len(labels_list)\n",
        "    r[labels_list.index(label)] = 1\n",
        "    res = np.array(r) if return_long else np.array(np.where(np.array(r) == 1)).tolist()[0]\n",
        "    return [res[0] / len(labels_list)] if normalize else res\n",
        "\n",
        "def save_test_labels_to_file(label_text_pairs, file_name='test_anotations_labels.jsons', size=1000):\n",
        "    ''' save labels to file '''\n",
        "    rNums = np.random.randint(len(label_text_pairs), size=size)\n",
        "    with open(file_name, 'w') as f:\n",
        "        for i in tqdm(rNums):\n",
        "            f.write('{ \"label\": \"%s\", \"text\": \"%s\" }\\n' % (label_text_pairs[i][0], label_text_pairs[i][1]))\n",
        "\n",
        "def save_labels_to_file(label_text_pairs, file_name='anotations_labels.jsons'):\n",
        "    ''' save labels to file '''\n",
        "    with open(file_name, 'w') as f:\n",
        "        for label, text in tqdm(label_text_pairs):\n",
        "            f.write('{ \"label\": \"%s\", \"text\": \"%s\" }\\n' % (label, text))\n",
        "\n",
        "feature_to_notes = {}\n",
        "for I in range(len(features)):\n",
        "    feature_to_notes[str(features['feature_num'][I])] = []\n",
        "\n",
        "# add notes to the dictionary\n",
        "for I in range(len(train)):\n",
        "    feature_to_notes[str(train['feature_num'][I])].extend(strtypeArrToArr(train['annotation'][I]))\n",
        "\n",
        "# get_words_only('felt dizzy, last meal was \"2\" days ago, weakness, ew$sdfg12 \"fatigu\"-re')\n",
        "label_text_pairs = []\n",
        "# label_text_pairs = [[key, get_words_only_str(note)] for note in feature_to_notes[key] for key in feature_to_notes]\n",
        "for key in feature_to_notes:\n",
        "    for note in feature_to_notes[key]:\n",
        "        label_text_pairs.append([key, get_words_only_str(note)])\n",
        "\n",
        "labels_list = list(feature_to_notes.keys())\n",
        "words_list = get_words_from_notes(patient_notes, train, save_file=False)\n",
        "features_to_notes = {}\n",
        "\n",
        "if True:\n",
        "    print(encode_to_arr(' '.join([words_list[1], words_list[2], words_list[-1]]), words_list))\n",
        "    print(encode_to_arr('felt as if he going to pass out', words_list))\n",
        "    print(encode_to_arr('felt as if he going to pass out', words_list, False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KlwEfbtDqn9",
        "outputId": "334aa133-fbdf-4020-e11b-89a2f9ea8ef1"
      },
      "outputs": [],
      "source": [
        "# x_trainA = [encode_to_arr(label_text[1], words_list )   for label_text in tqdm(label_text_pairs)]\n",
        "# y_trainA = [encode_labels(label_text[0], labels_list)   for label_text in tqdm(label_text_pairs)]\n",
        "\n",
        "# label_text_pairs = [label, text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.0'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "str(encode_labels(label_text_pairs[12][0], labels_list, return_long=False, normalize=True)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "04/10/2022 02:25:12 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
          ]
        }
      ],
      "source": [
        "from happytransformer import HappyTextClassification\n",
        "happy_tc_roberta = HappyTextClassification(\"ROBERTA\", \"roberta-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# cases= [(\"Wow I love using BERT for text classification\", 0), (\"I hate NLP\", 1)]\n",
        "def write_to_csv(cases, filename=\"train_classifier.csv\"):\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "            writer = csv.writer(csvfile)\n",
        "            writer.writerow([\"text\", \"label\"])\n",
        "            for case in cases:\n",
        "                writer.writerow([case[1], encode_labels(case[0], labels_list, return_long=False)[0]])\n",
        "    return filename\n",
        "\n",
        "happy_tc_roberta.train(write_to_csv(cases=label_text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# comments\n",
        "\n",
        "replacing all labels with some integer (tried 0 & 1 only) atleast starts the training without any errors. \n",
        "\n",
        "used : \n",
        "\n",
        "https://www.vennify.ai/train-text-classification-transformers/\n",
        "\n",
        "\n",
        "**smooth labeling?**\n",
        "\n",
        "\n",
        "https://towardsdatascience.com/what-is-label-smoothing-108debd7ef06\n",
        "\n",
        "https://pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
